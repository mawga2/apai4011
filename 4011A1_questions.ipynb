{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3sWPwqK_G9B"
   },
   "source": [
    "## APAI/STAT 4011 Natural Language Processing\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "### Submission format: 2 files (please don't zip them together), one is the ipynb file implemented with code and comments here, and one is pdf/ html file generated from this notebook. It's highly suggested that you directly write in this notebook.\n",
    "\n",
    "*The late submission policy*: If you have difficulty handing in on time (e.g., illness etc.), you would need to send the official certificate to Dr. Lau (and cc the tutor Tracy) at least one day before the deadline via email. Otherwise, any late submission is not allowed, and your grade would be counted as 0 for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VR8hT_v__qg"
   },
   "source": [
    "# Part 1: Starting point of the real-life analysis (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYAV75YrAc6q"
   },
   "source": [
    "In this assignment, you would need to perform sentimental analysis on the corpus ***twitter_samples*** from nltk library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gveuzy2pA5_g"
   },
   "source": [
    "### Q1-1. Load the data to your workspace and print out all the file ids inside twitter_samples. Put all the libraries or packages you would need to use in this assignment here. (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kebDVi3BJvj"
   },
   "outputs": [],
   "source": [
    "# some sample libraries are provided here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "# print out the file ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8ZHCjnlBdTK"
   },
   "source": [
    "### Q1-2. Load the negative and positive tweets file to a dataframe with column names {tweet, sentiment} and assign label as 0 for negative, 1 for positive. Print the first 3 rows of your dataframe. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9LKETvtDB1X"
   },
   "outputs": [],
   "source": [
    "your_dataframe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28KFMi2PDTge"
   },
   "source": [
    "### Q1-3. Visualize and comment on the data balance of this dataset. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T333yOFJGqhW"
   },
   "outputs": [],
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICgSmVCaGozi"
   },
   "source": [
    "Your comment here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj4YIVH2Gxj8"
   },
   "source": [
    "# Part 2: Text Preprocessing (25 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzlM-QS-LGEv"
   },
   "source": [
    "### Q2-1: The original tweets might be noisy, containing lots of elements that cause mess. Please think of what preprocessing procedure is needed for this specific dataset, and more important, what is the correct order between these steps. The below functions are the ones I think quite important for the data. Please fill in the function defintion, design more functions (if you want) and apply them in the order you decide. The text after processed should be save in a new column named **tidy_tweet**. Please clearly explain the reason you want to add this function, and why you would like to arrange the order like this. For each function, you need to print out the difference before and after the approach by showing the columns {tweet, tidy_tweet} side by side. (14 marks)\n",
    "\n",
    "Hint: you may need to find some specific tweets to show the difference. If you want to show that your remove_url function is working properly, you may need to find some original tweets containing urls. You may consider controlling this by the random_state parameter or other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wvULAdBG0Ob"
   },
   "outputs": [],
   "source": [
    "def convert_to_lowercase(tweet):\n",
    "    '''\n",
    "    aim: change all tweets to lower case\n",
    "    '''\n",
    "    return tweet\n",
    "\n",
    "def emoji_to_word(tweet):\n",
    "    '''\n",
    "    aim: remove all the emoji in the tweets\n",
    "    '''\n",
    "    return tweet\n",
    "\n",
    "def emoticons_to_word(tweet):\n",
    "    '''\n",
    "    aim: based on the emoticon dictionary, replace all the emoticons to corresponding word\n",
    "    The emoticon dictionary is provided in the next block\n",
    "    '''\n",
    "    return tweet\n",
    "\n",
    "def remove_pattern(tweet, pattern):\n",
    "    '''\n",
    "    aim: remove all the \"@users\" appears in the tweets\n",
    "    '''\n",
    "    return tweet\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    '''\n",
    "    aim: remove all the punctuation from the tweet given\n",
    "    Punctuations are characters other than alphaters and digits.\n",
    "    '''\n",
    "    return tweet\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    '''\n",
    "    aim: remove all stopwords in the tweets\n",
    "    '''\n",
    "    return tweet\n",
    "\n",
    "def remove_urls(tweet):\n",
    "    '''\n",
    "    aim: remove all the urls contained inside the tweets\n",
    "    '''\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMLdp1glNQv5"
   },
   "outputs": [],
   "source": [
    "Emoticon_Dictionary = {\n",
    "    u\":‑\\)\":\"smiley\",\n",
    "    u\":\\)\":\"smiley\",\n",
    "    u\":-\\]\":\"smiley\",\n",
    "    u\":\\]\":\"smiley\",\n",
    "    u\":-3\":\"smiley\",\n",
    "    u\":3\":\"smiley\",\n",
    "    u\":->\":\"smiley\",\n",
    "    u\":>\":\"smiley\",\n",
    "    u\"8-\\)\":\"smiley\",\n",
    "    u\":o\\)\":\"smiley\",\n",
    "    u\":-\\}\":\"smiley\",\n",
    "    u\":\\}\":\"smiley\",\n",
    "    u\":-\\)\":\"smiley\",\n",
    "    u\":c\\)\":\"smiley\",\n",
    "    u\":\\^\\)\":\"smiley\",\n",
    "    u\"=\\]\":\"smiley\",\n",
    "    u\"=\\)\":\"smiley\",\n",
    "    u\":‑D\":\"Laughing\",\n",
    "    u\":D\":\"Laughing\",\n",
    "    u\"8‑D\":\"Laughing\",\n",
    "    u\"8D\":\"Laughing\",\n",
    "    u\"X‑D\":\"Laughing\",\n",
    "    u\"XD\":\"Laughing\",\n",
    "    u\"=D\":\"Laughing\",\n",
    "    u\"=3\":\"Laughing\",\n",
    "    u\"B\\^D\":\"Laughing\",\n",
    "    u\":-\\)\\)\":\"happy\",\n",
    "    u\":‑\\(\":\"sad\",\n",
    "    u\":-\\(\":\"sad\",\n",
    "    u\":\\(\":\"sad\",\n",
    "    u\":‑c\":\"sad\",\n",
    "    u\":c\":\"sad\",\n",
    "    u\":‑<\":\"sad\",\n",
    "    u\":<\":\"sad\",\n",
    "    u\":‑\\[\":\"sad\",\n",
    "    u\":\\[\":\"sad\",\n",
    "    u\":-\\|\\|\":\"sad\",\n",
    "    u\">:\\[\":\"sad\",\n",
    "    u\":\\{\":\"sad\",\n",
    "    u\":@\":\"sad\",\n",
    "    u\">:\\(\":\"sad\",\n",
    "    u\":'‑\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‑\\)\":\"happiness\",\n",
    "    u\":'\\)\":\"happiness\",\n",
    "    u\"D‑':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"dismay\",\n",
    "    u\"D;\":\"dismay\",\n",
    "    u\"D=\":\"dismay\",\n",
    "    u\"DX\":\"dismay\",\n",
    "    u\":‑O\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‑o\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‑0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‑\\)\":\"smirk\",\n",
    "    u\";\\)\":\"smirk\",\n",
    "    u\"\\*-\\)\":\"smirk\",\n",
    "    u\"\\*\\)\":\"smirk\",\n",
    "    u\";‑\\]\":\"smirk\",\n",
    "    u\";\\]\":\"smirk\",\n",
    "    u\";\\^\\)\":\"smirk\",\n",
    "    u\":‑,\":\"smirk\",\n",
    "    u\";D\":\"smirk\",\n",
    "    u\":‑P\":\"playful\",\n",
    "    u\":P\":\"playful\",\n",
    "    u\"X‑P\":\"playful\",\n",
    "    u\"XP\":\"playful\",\n",
    "    u\":‑Þ\":\"playful\",\n",
    "    u\":Þ\":\"playful\",\n",
    "    u\":b\":\"playful\",\n",
    "    u\"d:\":\"playful\",\n",
    "    u\"=p\":\"playful\",\n",
    "    u\">:P\":\"playful\",\n",
    "    u\":‑/\":\"annoyed\",\n",
    "    u\":/\":\"annoyed\",\n",
    "    u\":-[.]\":\"annoyed\",\n",
    "    u\">:[(\\\\\\)]\":\"annoyed\",\n",
    "    u\">:/\":\"annoyed\",\n",
    "    u\":[(\\\\\\)]\":\"annoyed\",\n",
    "    u\"=/\":\"annoyed\",\n",
    "    u\"=[(\\\\\\)]\":\"annoyed\",\n",
    "    u\":L\":\"annoyed\",\n",
    "    u\"=L\":\"annoyed\",\n",
    "    u\":S\":\"annoyed\",\n",
    "    u\":‑\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed\",\n",
    "    u\":‑x\":\"tongue-tied\",\n",
    "    u\":x\":\"tongue-tied\",\n",
    "    u\":‑#\":\"tongue-tied\",\n",
    "    u\":#\":\"tongue-tied\",\n",
    "    u\":‑&\":\"tongue-tied\",\n",
    "    u\":&\":\"tongue-tied\",\n",
    "    u\"O:‑\\)\":\"innocent\",\n",
    "    u\"O:\\)\":\"innocent\",\n",
    "    u\"0:‑3\":\"innocent\",\n",
    "    u\"0:3\":\"innocent\",\n",
    "    u\"0:‑\\)\":\"innocent\",\n",
    "    u\"0:\\)\":\"innocent\",\n",
    "    u\":‑b\":\"cheeky\",\n",
    "    u\"0;\\^\\)\":\"innocent\",\n",
    "    u\">:‑\\)\":\"Evil\",\n",
    "    u\">:\\)\":\"Evil\",\n",
    "    u\"\\}:‑\\)\":\"Evil\",\n",
    "    u\"\\}:\\)\":\"Evil\",\n",
    "    u\"3:‑\\)\":\"Evil\",\n",
    "    u\"3:\\)\":\"Evil\",\n",
    "    u\">;\\)\":\"Evil\",\n",
    "    u\"\\|;‑\\)\":\"Cool\",\n",
    "    u\"\\|‑O\":\"Bored\",\n",
    "    u\":‑J\":\"Tongue-in-cheek\",\n",
    "    u\"#‑\\)\":\"Party\",\n",
    "    u\"%‑\\)\":\"confused\",\n",
    "    u\"%\\)\":\"confused\",\n",
    "    u\":-###..\":\"sick\",\n",
    "    u\":###..\":\"sick\",\n",
    "    u\"<:‑\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"respect\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"respect\",\n",
    "    u\"<\\(_ _\\)>\":\"respect\",\n",
    "    u\"<m\\(__\\)m>\":\"respect\",\n",
    "    u\"m\\(__\\)m\":\"respect\",\n",
    "    u\"m\\(_ _\\)m\":\"respect\",\n",
    "    u\"\\('_'\\)\":\"Sad\",\n",
    "    u\"\\(/_;\\)\":\"Sad\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad\",\n",
    "    u\"\\(;_;\":\"Sad\",\n",
    "    u\"\\(;_:\\)\":\"Sad\",\n",
    "    u\"\\(;O;\\)\":\"Sad\",\n",
    "    u\"\\(:_;\\)\":\"Sad\",\n",
    "    u\"\\(ToT\\)\":\"Sad\",\n",
    "    u\";_;\":\"Sad\",\n",
    "    u\";-;\":\"Sad\",\n",
    "    u\";n;\":\"Sad\",\n",
    "    u\";;\":\"Sad\",\n",
    "    u\"Q\\.Q\":\"Sad\",\n",
    "    u\"T\\.T\":\"Sad\",\n",
    "    u\"QQ\":\"Sad\",\n",
    "    u\"Q_Q\":\"Sad\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(一一\\)\":\"Shame\",\n",
    "    u\"\\(；一_一\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling\",\n",
    "    u\"\\(\\・\\・?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Laugh\",\n",
    "    u\"\\^/\\^\":\"Laugh\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Laugh\",\n",
    "    u\"\\(\\^—\\^\\）\":\"Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Laugh\",\n",
    "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(ーー;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‘A`\\)\":\"Snubbed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glfRs2b2eDRh"
   },
   "outputs": [],
   "source": [
    "# following the order you specify above, print out columns {tweet, tidy_tweet} side by side\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g9DSanZ04M-"
   },
   "source": [
    "### Q2-2: After obtaining the clean text, we would perform stemming or lemmatization on the tweets. Fill in the defintion below and compare the difference between these two methods by creating and printing two columns {stem_tweet, lemm_tweet} side by side. State the reason why you prefer one over another. Choose the one you prefer to apply the procedure on the column {tidy_tweet}. (6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qym5kaZK7952"
   },
   "outputs": [],
   "source": [
    "def stemming(tweet):\n",
    "    '''\n",
    "    aim: perform stemming on the text\n",
    "    '''\n",
    "\n",
    "def lemmatization(tweet):\n",
    "    '''\n",
    "    aim: perform lemmatization on the text\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf6jiEHRAK0O"
   },
   "source": [
    "### Q2-3: Tokenize the tweets and print out the final dataframe (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqVNOv9XBHBG"
   },
   "outputs": [],
   "source": [
    "def tokenization(tweet):\n",
    "    '''\n",
    "    aim: perform tokenization on the text\n",
    "    '''\n",
    "\n",
    "your_final_dataframe.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRIPsyYXBi2_"
   },
   "source": [
    "# Part 3: Visualization (15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT7mlYNvB9zM"
   },
   "source": [
    "### Q3-1: Build and plot the wordcloud for the whole dataset, the negative tweets, and the positive tweets, respectively. Analyze if the result is reasonable. Or if you think there are strange scenarios, what is the possible reasons behind. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8oXbR9ZDAi8"
   },
   "outputs": [],
   "source": [
    "# wordcloud for all\n",
    "\n",
    "# wordcloud for negative\n",
    "\n",
    "# wordcloud for positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdkyXoGUDJb-"
   },
   "source": [
    "### Q3-2: Hashtag might contains important information regarding the sentiment. Build two barplots for the hashtag in negative tweets and positive tweets respectively. What do you find? Do you think hashtag is helpful for our sentimental analysis? (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nB8810dHd7b"
   },
   "outputs": [],
   "source": [
    "# bar plot for negative\n",
    "\n",
    "# bar plot for positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cTSbuR1Hl3g"
   },
   "source": [
    "# Part 4: Modelling (50 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOBECk2RMiGq"
   },
   "source": [
    "### Q4-1: Divide the dataset into 80% training set and 20% testing set. Perform three types of feature extraction techniques we have covered so far to the dataset for preparation. Clearly state what is the difference between these techniques and which one you expect would be the most appropriate. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jm1CmHcoNB9x"
   },
   "outputs": [],
   "source": [
    "# divide the datatset\n",
    "\n",
    "# feature extraction 1\n",
    "\n",
    "# feature extraction 2\n",
    "\n",
    "# feature extraction 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tc3Z5QAQ7Sc"
   },
   "source": [
    "### Q4-2: Use the sklearn package to implement KNN classifier models with difference number of neighbors. Please use three types of features above, and use the following metrics for comparing the performance: {accuracy, f1, precision, recall}. Write out how the metrics are calculated with formulas (or other ways). Plot the results using line chart, and suggest the best choice for this parameter. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JS5yi3DmR5bg"
   },
   "outputs": [],
   "source": [
    "# KNN classifiers with different neighbors (1,2,3,4,... etc)\n",
    "\n",
    "# plot a line chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqMKFMG4IL_0"
   },
   "source": [
    "### Q4-3: Use the sklearn package to implement Naive Bayes classifier, Logistic regression, and KNN classifier. For KNN, you may directly use the best result above. Again use three types of features above, and use the following metrics for comparing the performance: {accuracy, f1, precision, recall}. Summarize the scores into tables and provide possible reasons that lead to this situation. Is the performance good? Why? (5 marks)\n",
    "\n",
    "Hint: you should have $3*3*4=36$ outputs. When performing comparison, you could analyze from two views, one is comparing different feature extraction methods while model is the same, another is comparing different models when applying the same feature extraction procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wf_gHR5KrEI"
   },
   "outputs": [],
   "source": [
    "# naive bayes classifier\n",
    "\n",
    "# logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH5UVzfdLFLn"
   },
   "source": [
    "### Q4-4: Write out the procedure how naive bayes classifier is classifying our dataset. Remember to clearly write out the mathematical formulas involved in the procedure. Implement the naive bayes classifier step by step (without directly calling any package). Add clear explanations to your python code based on the procedure you stated above. Compare the results you get here with the one in Q4-3. (15 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OxoeOwKMcse"
   },
   "source": [
    "Naive Bayes procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSnAs2NdOn7t"
   },
   "outputs": [],
   "source": [
    "# implement naive bayes based on the above procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4UQJfZlS2xy"
   },
   "source": [
    "### Q4-5: Write out the procedure how logistic regression is working in our case. Remember to clearly write out the mathematical formulas involved in the procedure. Implement the logistic regression using Stochastic Gradient Descent (SGD) step by step (without directly calling any package). Add clear explanations to your python code based on the procedure you stated above. Compare the results you get here with the one in Q4-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsVOTBi5Vuzq"
   },
   "source": [
    "Logistic regression procedure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msor7ylEUraQ"
   },
   "outputs": [],
   "source": [
    "# implement logistic regression based on the above procedure\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
